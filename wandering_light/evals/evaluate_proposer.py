import json
import os
import random
from dataclasses import asdict, dataclass, field
from datetime import datetime
from typing import Any

import fire

from wandering_light.constants import DEFAULT_EVAL_FILE
from wandering_light.evals.evaluate_solver import EvaluateSolver
from wandering_light.function_def import FunctionDefList, FunctionDefSet
from wandering_light.llm_utils import generate_proposer_training_prompt
from wandering_light.solver import (
    TokenGenerator,
    TrainedLLMTokenGenerator,
    TrajectorySolver,
    create_token_solver,
)
from wandering_light.trajectory import (
    Trajectory,
    TrajectoryList,
    TrajectorySpec,
    TrajectorySpecList,
)
from wandering_light.typed_list import TypedList


@dataclass
class SampleResult:
    raw_response: str
    parse_success: bool
    problem_spec: TrajectorySpec
    attempted_function_deflists: list[FunctionDefList]
    solve_rate: float

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization compatibility."""
        return {
            "raw_response": self.raw_response,
            "parse_success": self.parse_success,
            "problem_spec": str(self.problem_spec) if self.problem_spec else None,
            "attempted_function_deflists": [
                [f.name for f in deflist]
                for deflist in self.attempted_function_deflists
            ],
            "solve_rate": self.solve_rate,
        }


@dataclass
class EvalResult:
    parse_rate: float
    avg_function_count: float
    avg_function_count_ratio: float
    solver_success_rate: float
    num_samples: int
    frac_non_zero_std: float = 0.0
    sample_results: list[SampleResult] = field(default_factory=list)

    def __str__(self):
        return f"EvalResult(parse_rate={self.parse_rate:.2f}, avg_function_count={self.avg_function_count:.2f}, avg_function_count_ratio={self.avg_function_count_ratio:.2f}, solver_success_rate={self.solver_success_rate:.2f}, num_samples={self.num_samples}, frac_non_zero_std={self.frac_non_zero_std:.2f})"

    def to_dict(self) -> dict[str, Any]:
        """Convert the evaluation result to a dictionary for serialization."""
        result_dict = asdict(self)
        # Convert sample_results to list of dicts for JSON serialization
        result_dict["sample_results"] = [
            sample.to_dict() for sample in self.sample_results
        ]
        return result_dict

    def save_to_file(self, output_file: str):
        """Save the evaluation result to a JSON file."""
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, "w") as f:
            json.dump(self.to_dict(), f, indent=2)


def evaluate_proposer(
    model: TokenGenerator,
    trajectory_solver: TrajectorySolver | None,
    trajectories: list[Trajectory],
    num_samples: int,
    solver_attempts: int = 8,
    seed: int = 42,
    save_results: bool = False,
    filename: str | None = None,
    output_dir: str = "results/proposer/",
) -> EvalResult:
    """Evaluate the proposer model on the given trajectories.

    Args:
        model: The proposer model (TokenGenerator) to evaluate.
        trajectory_solver: The solver model (TrajectorySolver) to use for evaluation.
        trajectories: The trajectories to evaluate the proposer model on.
        num_samples: The number of samples to evaluate the model on.
        solver_attempts: The number of samples generated by the solver model for each problem.
        seed: The seed for the random number generator.
        save_results: whether to save detailed results to a file
        filename: filename to save results if save_results is True. If None, a timestamp will be used.
        output_dir: directory to save results if save_results is True

    Returns:
        EvalResult: The evaluation result.
    """
    if num_samples == 0 or not trajectories:
        return EvalResult(
            parse_rate=0.0,
            avg_function_count=0.0,
            avg_function_count_ratio=0.0,
            solver_success_rate=0.0,
            num_samples=num_samples,
            sample_results=[],
        )

    # Extract available functions from all trajectories
    available_functions = FunctionDefSet()
    for trajectory in trajectories:
        available_functions.extend(trajectory.function_defs)

    random.seed(seed)
    list_trajectories = list(trajectories)

    # Generate all prompts first for batch processing
    prompts = []
    for _i in range(num_samples):
        # Generate prompt for the model
        few_shot_examples = random.sample(list_trajectories, 3)
        training_example = generate_proposer_training_prompt(
            example_trajectories=few_shot_examples,
            target_spec=TrajectorySpec(
                input_list=TypedList([], item_type=int),
                function_defs=FunctionDefList([]),
            ),  # Not used for evaluation
            available_functions=available_functions,
        )
        prompts.append(training_example.input_text)

    # Get batch model predictions
    responses = model.generate_batch(prompts)
    result = evaluate_responses(
        responses, available_functions, trajectory_solver, solver_attempts, num_samples
    )

    if save_results:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"{filename or timestamp}.json")
        result.save_to_file(output_file)
        print(f"Saved detailed evaluation results to {output_file}")

    return result


def evaluate_responses(
    responses: list[str],
    available_functions: FunctionDefSet,
    solver_model: TrajectorySolver | None,
    solver_attempts: int,
    num_samples: int | None,
) -> EvalResult:
    # Initialize counters
    parsed_count = 0
    total_function_count = 0
    sample_results = []
    num_samples = num_samples or len(responses)

    generated_specs = TrajectorySpecList([])
    # Process all responses
    for response in responses:
        # Try to parse the response
        try:
            parsed_spec = TrajectorySpec.parse_from_string(
                response, available_functions
            )
            parsed_count += 1
            total_function_count += len(parsed_spec.function_defs)
            generated_specs.append(parsed_spec)
            sample_results.append(
                SampleResult(
                    raw_response=response,
                    parse_success=True,
                    problem_spec=parsed_spec,
                    attempted_function_deflists=[],
                    solve_rate=0.0,
                )
            )
        except ValueError:
            sample_results.append(
                SampleResult(
                    raw_response=response,
                    parse_success=False,
                    problem_spec=None,
                    attempted_function_deflists=[],
                    solve_rate=0.0,
                )
            )
    parse_rate = parsed_count / num_samples if num_samples > 0 else 0.0
    avg_function_count = (
        total_function_count / parsed_count if parsed_count > 0 else 0.0
    )

    frac_non_zero_std = 0.0
    # Evaluate the solver model
    if solver_model is not None:
        solver_eval_result = EvaluateSolver.evaluate_using_trajectory_specs(
            solver_model,
            TrajectorySpecList(
                [specs for specs in generated_specs for _ in range(solver_attempts)]
            ),  # Repeat each spec solver_attempts times
            available_functions,
            save_results=False,
            output_dir="",
        )

        avg_function_count_ratio = (
            solver_eval_result.avg_solution_length / avg_function_count
            if avg_function_count > 0
            else 0.0
        )
        solver_success_rate = solver_eval_result.success_rate

        # Collect per sample results
        non_zero_variance_count = 0
        j = 0
        for i in range(len(generated_specs)):
            # find the next parsed sample result
            while j < len(sample_results) and not sample_results[j].parse_success:
                j += 1
            if j < len(sample_results):
                results = solver_eval_result.detailed_results[
                    i * solver_attempts : i * solver_attempts + solver_attempts
                ]
                sample_results[j].attempted_function_deflists = [
                    (
                        available_functions.parse_string(
                            ",".join(res.predicted_functions)
                        )
                        if res.success
                        else FunctionDefList()
                    )
                    for res in results
                ]
                sample_results[j].solve_rate = (
                    float(sum(res.success for res in results) / len(results))
                    if len(results) > 0
                    else 0.0
                )
                non_zero_variance_count += (
                    1 if 0.0 < sample_results[j].solve_rate < 1.0 else 0
                )
                j += 1
        if len(generated_specs) > 0:
            frac_non_zero_std = float(non_zero_variance_count / len(generated_specs))
    else:
        avg_function_count_ratio = 0.0
        solver_success_rate = 0.0

    return EvalResult(
        parse_rate=parse_rate,
        avg_function_count=avg_function_count,
        avg_function_count_ratio=avg_function_count_ratio,
        solver_success_rate=solver_success_rate,
        num_samples=num_samples,
        frac_non_zero_std=frac_non_zero_std,
        sample_results=sample_results,
    )


def file_evaluate_proposer(
    model: str,
    solver_model: str,
    eval_file: str = DEFAULT_EVAL_FILE,
    num_samples: int | None = None,
    save_results: bool = False,
    filename: str | None = None,
    output_dir: str = "results/proposer/",
) -> EvalResult:
    """Evaluate the proposer model on the given trajectories.

    Args:
        model: The proposer model to evaluate.
        solver_model: The solver model to use for evaluation.
        eval_file: The file to evaluate the model on.
        num_samples: The number of samples to evaluate the model on.
        save_results: whether to save detailed results to a file
        filename: filename to save results if save_results is True. If None, a timestamp will be used.
        output_dir: directory to save results if save_results is True
    """
    if isinstance(model, str):
        model = TrainedLLMTokenGenerator(model)
    if isinstance(solver_model, str):
        solver_model = create_token_solver(
            TrainedLLMTokenGenerator(solver_model), budget=1
        )

    trajectories = TrajectoryList.from_file(eval_file)
    num_samples = num_samples or len(trajectories)
    result = evaluate_proposer(
        model=model,
        trajectory_solver=solver_model,
        trajectories=trajectories,
        num_samples=num_samples,
        save_results=save_results,
        filename=filename,
        output_dir=output_dir,
    )
    return result


if __name__ == "__main__":
    fire.Fire(file_evaluate_proposer)
